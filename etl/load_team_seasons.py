"""
Team-season hub and satellite loaders.

Implements Phase 2 loading for:
- team_season (hub)
- team_season_totals
- team_season_per_game
- team_season_per100
- team_season_opponent_totals
- team_season_opponent_per_game
- team_season_opponent_per100

Rules:
- team_season keyed by surrogate team_season_id (generated by DB).
- Ensure FKs to teams and seasons using ID resolution helpers.
- Honor league-average rows (team_id NULL, is_league_average = true).
"""

from __future__ import annotations

import os
from typing import Optional, Set

import polars as pl
from psycopg import Connection

from .config import Config
from .db import copy_from_polars, truncate_table
from .id_resolution import (
    SeasonLookup,
    TeamLookup,
    build_season_lookup,
    build_team_lookup,
    resolve_season_id,
    resolve_team_id_from_abbrev,
)
from .logging_utils import get_logger, log_structured
from .paths import (
    TEAM_OPP_PER100_CSV,
    TEAM_OPP_PER_GAME_CSV,
    TEAM_OPP_TOTALS_CSV,
    TEAM_PER100_CSV,
    TEAM_PER_GAME_CSV,
    TEAM_TOTALS_CSV,
    resolve_csv_path,
)

logger = get_logger(__name__)


def _read_csv_if_exists(path: str) -> Optional[pl.DataFrame]:
    if not os.path.exists(path):
        logger.warning("CSV missing; skipping", extra={"path": path})
        return None
    return pl.read_csv(path)


def _load_team_and_season_dims(conn: Connection) -> tuple[pl.DataFrame, pl.DataFrame]:
    with conn.cursor() as cur:
        cur.execute("SELECT team_id, team_abbrev FROM teams")
        teams = pl.from_records(
            cur.fetchall(),
            schema=["team_id", "team_abbrev"],
        )

        cur.execute("SELECT season_id, season_end_year, lg FROM seasons")
        seasons = pl.from_records(
            cur.fetchall(),
            schema=["season_id", "season_end_year", "lg"],
        )

    return teams, seasons


def load_team_season_hub(config: Config, conn: Connection) -> None:
    """
    Build team_season hub from team stats CSVs.
    Uses TEAM_TOTALS_CSV as primary source if present.
    """
    totals_path = resolve_csv_path(config, TEAM_TOTALS_CSV)
    totals = _read_csv_if_exists(totals_path)
    if totals is None:
        logger.warning("team_season hub load skipped: teamstats.csv not found")
        return

    teams_df, seasons_df = _load_team_and_season_dims(conn)
    team_lu = build_team_lookup(teams_df)
    season_lu = build_season_lookup(seasons_df)

    df = totals.rename(
        {
            "season": "season_end_year",
            "tm": "team_abbrev",
            "lg": "lg",
        }
    )

    # League average rows, if present, often flagged via team_abbrev or text.
    df = df.with_columns(
        pl.when(pl.col("team_abbrev").str.to_lowercase() == "league average")
        .then(True)
        .otherwise(False)
        .alias("is_league_average"),
        pl.lit(False).alias("is_playoffs"),
    )

    # Resolve season_id
    def _resolve_season(row) -> Optional[int]:
        return resolve_season_id(
            int(row["season_end_year"]),
            row.get("lg"),
            season_lu,
        )

    df = df.with_columns(
        pl.struct(["season_end_year", "lg"])
        .map_elements(_resolve_season, return_dtype=pl.Int64)
        .alias("season_id")
    )

    # Resolve team_id (NULL for league average rows)
    def _resolve_team(row) -> Optional[int]:
        if row["is_league_average"]:
            return None
        return resolve_team_id_from_abbrev(
            row.get("team_abbrev"),
            row.get("season_end_year"),
            team_lu,
        )

    df = df.with_columns(
        pl.struct(["team_abbrev", "season_end_year", "is_league_average"])
        .map_elements(_resolve_team, return_dtype=pl.Int64)
        .alias("team_id")
    )

    # Prepare minimal hub columns; let DB assign team_season_id via INSERT..RETURNING.
    hub_cols = [
        "team_id",
        "season_id",
        "season_end_year",
        "lg",
        "is_playoffs",
        "is_league_average",
        "team_abbrev",
    ]
    for col in hub_cols:
        if col not in df.columns:
            df = df.with_columns(pl.lit(None).alias(col))

    # Rebuild hub table by inserting and capturing IDs into a temp table
    truncate_table(conn, "team_season", cascade=True)

    # Build a temp frame of distinct hub keys
    hubs = (
        df.select(hub_cols)
        .unique()
        .with_columns(
            pl.arange(0, pl.count()).alias("_row_id"),
        )
    )

    # Copy hubs into a temporary table to get team_season_id
    temp_table = "tmp_team_season_hub"
    with conn.cursor() as cur:
        cur.execute(f"DROP TABLE IF EXISTS {temp_table}")
        cur.execute(
            f"""
            CREATE TEMP TABLE {temp_table} (
                _row_id BIGINT,
                team_id INTEGER,
                season_id BIGINT,
                season_end_year INTEGER,
                lg TEXT,
                is_playoffs BOOLEAN,
                is_league_average BOOLEAN,
                team_abbrev TEXT
            ) ON COMMIT DROP
            """
        )

    copy_from_polars(
        hubs.select(
            [
                "_row_id",
                "team_id",
                "season_id",
                "season_end_year",
                "lg",
                "is_playoffs",
                "is_league_average",
                "team_abbrev",
            ]
        ),
        temp_table,
        conn,
    )

    # Insert into team_season and capture mapping from hub keys to team_season_id
    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO team_season (
                team_id,
                season_id,
                season_end_year,
                lg,
                is_playoffs,
                is_league_average
            )
            SELECT
                team_id,
                season_id,
                season_end_year,
                lg,
                COALESCE(is_playoffs, FALSE),
                COALESCE(is_league_average, FALSE)
            FROM (
                SELECT DISTINCT
                    team_id,
                    season_id,
                    season_end_year,
                    lg,
                    is_playoffs,
                    is_league_average
                FROM tmp_team_season_hub
            ) s
            RETURNING
                team_id,
                season_id,
                season_end_year,
                is_playoffs,
                is_league_average,
                team_season_id
            """
        )
        rows = cur.fetchall()

    # Build lookup: (team_id, season_id, is_playoffs, is_league_average) -> team_season_id
    ts_map = {}
    for (
        team_id,
        season_id,
        season_end_year,
        is_playoffs,
        is_league_avg,
        ts_id,
    ) in rows:
        key = (team_id, season_id, season_end_year, bool(is_playoffs), bool(is_league_avg))
        ts_map[key] = int(ts_id)

    log_structured(
        logger,
        logger.level,
        "Loaded team_season hub",
        rows=len(rows),
    )

    # Attach team_season_id back to totals df for satellites
    def _lookup_team_season(row) -> Optional[int]:
        key = (
            row.get("team_id"),
            row.get("season_id"),
            row.get("season_end_year"),
            False,
            bool(row.get("is_league_average")),
        )
        return ts_map.get(key)

    df = df.with_columns(
        pl.struct(
            [
                "team_id",
                "season_id",
                "season_end_year",
                "is_league_average",
            ]
        )
        .map_elements(_lookup_team_season, return_dtype=pl.Int64)
        .alias("team_season_id")
    )

    # Cache for satellites
    _persist_team_season_id_map(conn, df)


def _persist_team_season_id_map(conn: Connection, df: pl.DataFrame) -> None:
    """
    Persist mapping of (season_end_year, team_abbrev) to team_season_id in a temp table
    for use by satellite loaders.
    """
    if "team_season_id" not in df.columns:
        return
    mapping = (
        df.select(
            [
                pl.col("season_end_year"),
                pl.col("team_abbrev"),
                pl.col("team_season_id"),
            ]
        )
        .drop_nulls("team_season_id")
        .unique()
    )
    if mapping.is_empty():
        return

    temp_table = "tmp_team_season_map"
    with conn.cursor() as cur:
        cur.execute(f"DROP TABLE IF EXISTS {temp_table}")
        cur.execute(
            f"""
            CREATE TEMP TABLE {temp_table} (
                season_end_year INTEGER,
                team_abbrev TEXT,
                team_season_id BIGINT
            ) ON COMMIT DROP
            """
        )

    copy_from_polars(mapping, temp_table, conn)
    log_structured(
        logger,
        logger.level,
        "Prepared tmp_team_season_map",
        rows=mapping.height,
    )


def _load_team_season_id_lookup(conn: Connection) -> dict[tuple[int, str], int]:
    temp_table = "tmp_team_season_map"
    lookup: dict[tuple[int, str], int] = {}
    with conn.cursor() as cur:
        cur.execute(
            f"""
            SELECT season_end_year, team_abbrev, team_season_id
            FROM {temp_table}
            """
        )
        for season_end_year, team_abbrev, ts_id in cur.fetchall():
            if season_end_year is None or team_abbrev is None or ts_id is None:
                continue
            key = (int(season_end_year), str(team_abbrev).upper())
            lookup[key] = int(ts_id)
    return lookup


def _attach_team_season_id(
    df: Optional[pl.DataFrame],
    lookup: dict[tuple[int, str], int],
    season_col: str = "season",
    team_abbrev_col: str = "tm",
) -> Optional[pl.DataFrame]:
    if df is None or df.is_empty():
        return None
    if season_col not in df.columns or team_abbrev_col not in df.columns:
        return None

    def _resolve(row) -> Optional[int]:
        season = row.get(season_col)
        abbr = row.get(team_abbrev_col)
        if season is None or not abbr:
            return None
        return lookup.get((int(season), str(abbr).upper()))

    return df.with_columns(
        pl.struct([season_col, team_abbrev_col])
        .map_elements(_resolve, return_dtype=pl.Int64)
        .alias("team_season_id")
    ).drop_nulls("team_season_id")


def _load_team_season_satellite(
    config: Config,
    conn: Connection,
    csv_name: str,
    table_name: str,
) -> None:
    path = resolve_csv_path(config, csv_name)
    df = _read_csv_if_exists(path)
    if df is None:
        logger.warning("%s load skipped: CSV not found", extra={"table": table_name, "csv": csv_name})
        return

    lookup = _load_team_season_id_lookup(conn)
    df = _attach_team_season_id(df, lookup)
    if df is None or df.is_empty():
        logger.info("No rows for %s after resolving team_season_id; skipping", extra={"table": table_name})
        return

    truncate_table(conn, table_name)
    copy_from_polars(df, table_name, conn)
    log_structured(
        logger,
        logger.level,
        "Loaded %s" % table_name,
        rows=df.height,
    )


def load_team_season_satellites(config: Config, conn: Connection) -> None:
    _load_team_season_satellite(config, conn, TEAM_TOTALS_CSV, "team_season_totals")
    _load_team_season_satellite(config, conn, TEAM_PER_GAME_CSV, "team_season_per_game")
    _load_team_season_satellite(config, conn, TEAM_PER100_CSV, "team_season_per100")
    _load_team_season_satellite(
        config,
        conn,
        TEAM_OPP_TOTALS_CSV,
        "team_season_opponent_totals",
    )
    _load_team_season_satellite(
        config,
        conn,
        TEAM_OPP_PER_GAME_CSV,
        "team_season_opponent_per_game",
    )
    _load_team_season_satellite(
        config,
        conn,
        TEAM_OPP_PER100_CSV,
        "team_season_opponent_per100",
    )


def load_all_team_seasons(config: Config, conn: Connection) -> None:
    """
    Orchestrate team-season hub and satellites.
    """
    load_team_season_hub(config, conn)
    load_team_season_satellites(config, conn)